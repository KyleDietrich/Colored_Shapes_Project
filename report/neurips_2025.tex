\documentclass{article}
\usepackage[final]{neurips_2025}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\title{Color-Shape Representation Learning with Autoencoders and Contrastive Learning}
\author{%
    Kyle Dietrich \\
    \And
    Preston Hines \\
}
\begin{document}
\maketitle
\begin{abstract}
\end{abstract}
\section{Introduction}
This project aims to reproduce, analyze, and extend two experiments from \textit{Foundations of Computer Vision} on a toy dataset of colored geometric shapes comprised of circles, trianges, and squares.
Specifically:
\begin{itemize}
    \item An \textbf{autoencoder-based} approach that reveals how different layers capture color vs. shape.
    \item A \textbf{contrastive-learning-based} approach using different data augmentations designed to produce embeddings either sensitive to color or sensitive to shape
\end{itemize}
Each experiment uses a similar convolutional encoder, but each experiment has a different objective.
The autoencoder experiment is focused on reconstruction while the contrastive learning experiment is focused alignment-uniformity.
The goal is to replicate the experiments on a synthetic dataset of \textbf{32x32} images containing randomly placed, sized, rotated, and colored shapes.
\\\\
We will closely follow \textbf{Section 30.4} as well as \textbf{Section 30.10.2} from the textbook.
The paper will confirm the following findings:
\begin{itemize}
    \item Autoencoders capture different features at different layers.
    \item Contrastive embeddings can be guided to focus on color or shape dependent on the nature of data augmentation.
\end{itemize}
\section{Motivation}
Understanding how deep networks learn representations is a fundamental challenge in computer vision.
Autoencoders and contrastive methods are two influential approaches for unsupervised or self-supervised feature learning.
By Replicating these textbook examples, we will deepen our understanding of core concepts\textemdash
how encoders \textit{compress} data;
how embeddings reflect invariances introduced by data augmentations;
and how evaluating embeddings via nearest neighbors or classification helps reveal learned features.
\newpage
\section{Approach}
\subsection{Baseline}
The baseline approach will be the replication of the autoencoder experiments from \textbf{Section 20.4}:
\begin{itemize}
    \item A convolutional autoencoder with a 128-dimensional bottleneck.
    \item Train on 64,000 synthetic images of colored shapes (32x32) using the Adam optimizer
    \item Evaluate the learned embeddings with nearest-neighbor analysis for color or shape classification.
\end{itemize}
\textbf{Potential Limitations}: Autoencoders typically do not enforce explicit invariances other than reconstructing the input.
They might capture color or shape equally well or somtimes not,
depending on the bottleneck dimension.
They do not necessarily disentangle color vs. shape unless we examine the intermediate or final layers
\subsection{Advanced}
The advanced approach will be implementing the contrastive learning technique based on an alignment + uniformity objective from \textbf{Section 30.10.2}.
Specifically:
\begin{itemize}
    \item \textbf{Color-sensitive embedding}: Use random crops that preserve color but alter shape cues.
    \item \textbf{Shape-sensitive embedding}: Use random crops + strong color jitter to randomize color.
\end{itemize}
\textbf{Why this leads to better performance or different embeddings}: 
Contrastive approaches actively push embeddings of \textit{positive} pairs together and push all other embeddings apart.
This is achieved by controlling how we augment. By either keeping color consistent or shape cues consistent,
the network is directed to rely on certain features for the classification.
\subsection{Tools and Computational Resources}
The computational resources used for this project was a standard NVIDIA RTX 3070, as well as a CPU compute given the small dataset.
\\\\
Tools
\begin{itemize}
    \item \textbf{Programming Language}: Python 3
    \item \textbf{Libraries}: PyTorch for model training, NumPy/PIL for synthetic data creation Matplotlib for visualization.
\end{itemize}
\section{Evaluation}
\subsection{Data}
\subsection{Metrics}
\begin{itemize}
    \item Nearest-neighbor classification on color and shape labels.
    \item Visualization of 2D contrastive embeddings to see if points cluster by color or shape.
    \item Reconstruction Error for the autoencoder
\end{itemize}

\subsection{Results}
\section{Conclusion}
\subsection{Insights}
The paper illuminates that:
\begin{itemize}
    \item Data augmentation can explicitly bias embeddings to focus on desired features.
    \item Autoencoders, by default, learn reconstructions but do not inherently separate color/shape without deeper layer analysis.
    \item Contrastive learning’s alignment and uniformity objectives reveal an elegant way to impose invariances. 
\end{itemize}
\subsection{Workload}
The project involves:
\begin{itemize}
    \item Implementing data generation scripts.
    \item Implementing the autoencoder and contrastive-learning models in PyTorch.
    \item Running extended experiments (cropping vs. color jitter).
    \item Logging and analyzing results (nearest-neighbor accuracy, 2D embedding plots).
    \item Writing documentation and a final report. 
\end{itemize}
\subsection{Challenges}
\section{Duplication Statement}
This project reproduces experiments from Foundations of Computer Vision (Torralba et al., 2024). Online code for the textbook’s examples is not directly used; instead, we will implement everything from scratch. We will ensure our results and code are not merely copies of any existing implementations but a genuine re-implementation and extension (e.g., adjusting resolutions, analyzing color/shape classification). 
\newpage
\section*{References}
\medskip
{
\small
[1] Antonio Torralba, Phillip Isola, \& William T.\ Freeman. (2024). \textit{Foundations of Computer Vision.} The MIT Press. 
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
\section{Technical Appendices and Supplementary Material}
Technical appendices with additional results, figures, graphs and proofs may be submitted with the paper submission before the full submission deadline (see above), or as a separate PDF in the ZIP file below before the supplementary material deadline. There is no page limit for the technical appendices.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
